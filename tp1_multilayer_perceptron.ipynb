{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**TP1: Multilayer Perceptron**\n",
        "\n",
        "### **Introdução**\n",
        "Nesta atividade de programação, você implementará o Multilayer Perceptron (MLP) em Python usando apenas a biblioteca numpy. Esta implementação permitirá a criação de redes neurais com qualquer número de camadas. Este modelo será usado para resolver o problema de classificação de imagens de gatos. Ao final desta atividade, você terá uma rede neural profunda capaz de classificar se uma dada imagem é um gato ou não.\n",
        "\n",
        "### **Objetivo**\n",
        "\n",
        "O principal objetivo deste projeto é aprender a implementação do algoritmo MLP com um número flexível de camadas, principalmente a **Propagação das entradas** e a **Retropropagação dos erros**. Além disso, você implementará a função de ativação ReLU e ganhará experiência na comparação de modelos neurais de diferentes tamanhos.\n",
        "\n",
        "### **Instruções**\n",
        "\n",
        "As células onde você precisa escrever o código são destacadas com os seguintes comentários:\n",
        "\n",
        "```python\n",
        "### SEU CÓDIGO COMEÇA AQUI ### ≈x linhas\n",
        "### SEU CÓDIGO TERMINA AQUI ###\n",
        "```\n",
        "\n",
        "Escreva suas soluções apenas entre estes dois comentários. Observe que o comentário inicial dá uma ideia do número de linhas de código esperado na solução.\n",
        "\n",
        "Após cada célula deste tipo, haverá uma célula de teste seguida pelos resultados esperados, para que você possa saber se sua solução está correta."
      ],
      "metadata": {
        "id": "owgUva0hvM6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Parte 0: Importar bibliotecas**\n",
        "\n",
        "Nossa primeira tarefa é carregar a biblioteca numpy e outras bibliotecas auxiliares, que nos ajudarão a visualizar os dados e os resultados:\n",
        "\n",
        "- **numpy**: principal biblioteca de computação científica em Python.\n",
        "- **matplotlib**: principal biblioteca para plotagem de gráficos em Python.\n",
        "- **h5py**: ler conjuntos de dados no formato h5.\n",
        "- **PIL**: testar seu modelo com suas próprias imagens.\n",
        "\n",
        "**Observação**:\n",
        "Como visto em aula, precisamos inicializar os pesos MLP com valores aleatórios próximos de zero. Portanto, neste projeto, inicializamos o gerador de números aleatórios numpy (np.random.seed(1)) com uma semente fixa, para que seus resultados sejam iguais aos esperados."
      ],
      "metadata": {
        "id": "loAR-kK6zFw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import scipy\n",
        "from PIL import Image\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(1)"
      ],
      "metadata": {
        "id": "AVLFnDvM67UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Parte 1: Carregar e pré-processar o conjunto de dados**\n",
        "\n",
        "#### **1.1 Baixar o conjunto de dados**\n",
        "\n",
        "O conjunto de dados deste projeto contém:\n",
        "\n",
        "- Treinamento: $n_{tr}$ imagens rotuladas como Gato ($y=1$) ou Não-gato ($y=0$)\n",
        "- Teste: $n_{te}$ imagens rotuladas como Gato ($y=1$) ou Não-gato ($y=0$)\n",
        "\n",
        "Todas as imagens são quadradas e coloridas. Portanto, elas são representadas por matrizes numpy do formato `(num_px, num_px, 3)`, onde `num_px` é a largura e a altura da imagem e 3 se refere aos canais de cor (RGB - Red, Green, Blue)."
      ],
      "metadata": {
        "id": "G_bnGdtEedNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O 'train_catvnoncat.h5' 'https://lucasnfe.github.io/ufv-inf721/static_files/datasets/p1-regressao-logistica/train_catvnoncat.h5'\n",
        "!wget -O 'test_catvnoncat.h5' 'https://lucasnfe.github.io/ufv-inf721/static_files/datasets/p1-regressao-logistica/test_catvnoncat.h5'\n",
        "\n",
        "train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n",
        "train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # imagens do conjunto de treino\n",
        "train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # rótulos do conjunto de treino\n",
        "\n",
        "test_dataset = h5py.File('test_catvnoncat.h5', \"r\")\n",
        "test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # imagens do conjunto de teste\n",
        "test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # rótulos do conjunto de teste\n",
        "\n",
        "classes = np.array([b\"nao gato\", b\"gato\"]) # lista das classes, classe 0 -> 'não gato'; classe 1 -> 'gato'\n",
        "\n",
        "train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))"
      ],
      "metadata": {
        "id": "0kHPWJZ5eiAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2 Visualização de exemplos de conjuntos de dados**\n",
        "\n",
        "Cada linha de `train_set_x_orig` e `test_set_x_orig` é uma matriz numpy que representa uma imagem. Você pode visualizar um exemplo executando o código a seguir. Sinta-se à vontade para alterar o valor da variável `index` e executar novamente para ver outras imagens."
      ],
      "metadata": {
        "id": "RJgKPBK6fDQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizando uma imagem do conjunto de treinamento\n",
        "index = 13\n",
        "plt.imshow(train_set_x_orig[index])\n",
        "print (\"y = \" + str(train_set_y[0, index]) + \", é uma imagem de '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"'.\")"
      ],
      "metadata": {
        "id": "uOlFLknufDte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.3 Dimensionalidade do problema**\n",
        "\n",
        "Esta é a primeira célula onde você escreverá o código. Crie as seguintes variáveis:\n",
        "\n",
        "- `n_train` (número de exemplos de treinamento)\n",
        "- `n_test` (número de exemplos de teste)\n",
        "- `num_px` (altura e largura de uma imagem do conjunto de dados)\n",
        "\n",
        "Use a propriedade `shape` de matrizes numpy e lembre-se de que `train_set_x_orig` é uma matriz numpy com o formato `(m_train, num_px, num_px, 3)`."
      ],
      "metadata": {
        "id": "1NCtU4Hrgag5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### SEU CÓDIGO COMEÇA AQUI ### (≈3 linhas)\n",
        "\n",
        "### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "print (\"Number of training examples: n_train = \" + str(n_train))\n",
        "print (\"Number of testing examples: n_test = \" + str(n_test))\n",
        "print (\"Height/Width of the images: num_px = \" + str(num_px))\n",
        "print(\"---\")\n",
        "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
        "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))"
      ],
      "metadata": {
        "id": "OxOYVzsHgZLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Esperado**:\n",
        "<table style=\"width:15%\">\n",
        "  <tr>\n",
        "    <td><b>n_train<b></td>\n",
        "    <td>209</td>\n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td><b>n_test<b></td>\n",
        "    <td> 50 </td>\n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td><b>num_px<b></td>\n",
        "    <td> 64 </td>\n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ],
      "metadata": {
        "id": "wDftttcV07m9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.4 Pré-processamento dos dados**\n",
        "\n",
        "#### **1.4.1 Transformando imagens em vetores de características**\n",
        "\n",
        "Para processar as imagens do conjunto de dados com regressão logística, precisamos transformá-las em vetores de características. Para isso, achataremos essas imagens, originalmente com o formato `(num_px, num_px, 3)`, em vetores com o formato `(num_px * num_px * 3, 1)`.\n",
        "\n",
        "\n",
        "Usaremos a função reshape dos arrays numpy para modificar o formato das imagens de treinamento `(train_set_x_orig)` e de teste `(test_set_x_orig)` e, então, armazenaremos as imagens de treinamento e teste achatadas em novos arrays chamados `train_set_x_flatten` e `test_set_x_flatten`, respectivamente."
      ],
      "metadata": {
        "id": "jhDbjv8O0-Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_x_flatten = train_set_x_orig.reshape((n_train, -1)).T\n",
        "test_set_x_flatten = test_set_x_orig.reshape((n_test, -1)).T\n",
        "\n",
        "print (\"train_set_x_flatten tamanho: \" + str(train_set_x_flatten.shape))\n",
        "print (\"train_set_y tamanho: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x_flatten tamanho: \" + str(test_set_x_flatten.shape))\n",
        "print (\"test_set_y tamanho: \" + str(test_set_y.shape))\n",
        "print (\"Primeiras 5 características (pixel values) do primeiro exemplo de treino: \" + str(train_set_x_flatten[0:5,0]))"
      ],
      "metadata": {
        "id": "itxjTWtz1DGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Esperado**:\n",
        "\n",
        "<table style=\"width:35%\">\n",
        "  <tr>\n",
        "    <td><b> train_set_x_flatten shape<b></td>\n",
        "    <td> (12288, 209)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><b>train_set_y shape<b></td>\n",
        "    <td>(1, 209)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><b> test_set_x_flatten shape<b></td>\n",
        "    <td>(12288, 50)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><b>test_set_y shape<b></td>\n",
        "    <td>(1, 50)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "  <td>First 5 features (pixel values) of the first training example</td>\n",
        "  <td>[17 31 56 22 33]</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "TE4Kza301EyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.4.2 Normalizar valores de pixel**\n",
        "\n",
        "Uma etapa muito comum de pré-processamento de imagens em aprendizado de máquina é normalizar valores de pixel entre 0 e 1. Para isso, basta dividir cada linha do conjunto de dados por 255 (o valor máximo de um canal de pixel). Divida as matrizes `train_set_x_flatten` e `test_set_x_flatten` de imagens achatadas por 255 e armazene os resultados em novas matrizes chamadas `train_set_x` e `test_set_x`."
      ],
      "metadata": {
        "id": "Uiw_qjMB1IMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### SEU CÓDIGO COMEÇA AQUI ### (≈2 linhas)\n",
        "\n",
        "### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "print (\"First 5 features of the normalized feature vector of the first training example: \" + str(train_set_x[0:5,0]))"
      ],
      "metadata": {
        "id": "-hnrf40V1LFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado Esperado**:\n",
        "\n",
        "<table style=\"width:35%\">\n",
        "  <tr>\n",
        "    <td>First 5 features of the normalized feature vector of the first training example</td>\n",
        "    <td> [0.06666667 0.12156863 0.21960784 0.08627451 0.12941176]</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "4RF3Zlki1L8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Parte 2: Definir o modelo**\n",
        "\n",
        "Neste projeto, usaremos o MLP para a classificação de imagens de gatos. Lembre-se de que o MLP organiza os neurônios em uma estrutura em camadas. A formulação geral do MLP permite que tanto o número de camadas L quanto o número de neurônios $m^{[l]}$ por camada $1 \\leq l \\leq L$ sejam configuráveis ​​de acordo com a complexidade do problema de aprendizado. Além disso, as funções de ativação dos neurônios do MLP não precisam necessariamente ser a função sigmoide. Podemos escolher a função de ativação para cada camada da rede. Lembre-se de que esta formulação geral do MLP tem a seguinte hipótese:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "Z^{[l]} &= W^{[l]} A^{[l-1]} + \\mathbf{b}^{[l]} \\\\\n",
        "\\mathbf{A}^{[l]} &= g^{[l]}(Z^{[l]}) \\\\\n",
        "\\end{align} $$, onde:\n",
        "\n",
        "- $W^{[l]}$ é a matriz de pesos da camada $l$ com dimensões $({m^{[l]}, m^{[l-1]}}$);\n",
        "- $\\mathbf{b}^{[l]}$ é o vetor de polarização da camada $l$ com dimensões $(m^{[l]}, 1)$;\n",
        "- $A^{[L]}$ é a matriz de ativação da camada $l$ com dimensões $(m^{[l]}, n)$;\n",
        "- $g^{[l]}$ é a função de ativação da camada $l$.\n",
        "\n",
        "De acordo com esta definição, teremos as seguintes dimensões para uma rede com L camadas (Lembre-se de que $A^{[0]} = X$ e $A^{[L]} = \\hat{Y}$):\n",
        "\n",
        "<table style=\"width:100%\">\n",
        "    <tr>\n",
        "        <td>  </td>\n",
        "        <td> <b>  Dimensão de $W^{[l]}$  <b> </td>\n",
        "        <td> <b>  Dimensão de $b^{[l]}$  <b>  </td>\n",
        "        <td> <b>  Dimensão de $A^{[l]}$  <b> </td>\n",
        "    <tr>\n",
        "    <tr>\n",
        "        <td> <b> Camada [1] <b> </td>\n",
        "        <td> $(m^{[1]}, 12288)$ </td>\n",
        "        <td> $(m^{[1]}, 1)$ </td>\n",
        "        <td> $(m^{[1]}, 209)$ </td>\n",
        "    <tr>\n",
        "    <tr>\n",
        "        <td> <b> Camada [2] <b> </td>\n",
        "        <td> $(m^{[2]}, m^{[1]})$  </td>\n",
        "        <td> $(m^{[2]}, 1)$ </td>\n",
        "        <td> $(m^{[2]}, 209)$ </td>\n",
        "    <tr>\n",
        "       <tr>\n",
        "        <td> $\\vdots$ </td>\n",
        "        <td> $\\vdots$  </td>\n",
        "        <td> $\\vdots$  </td>\n",
        "        <td> $\\vdots$  </td>\n",
        "    <tr>\n",
        "   <tr>\n",
        "        <td> <b> Camada [L-1] <b> </td>\n",
        "        <td> $(m^{[L-1]}, m^{[L-2]})$ </td>\n",
        "        <td> $(m^{[L-1]}, 1)$  </td>\n",
        "        <td> $(m^{[L-1]}, 209)$ </td>\n",
        "    <tr>\n",
        "   <tr>\n",
        "        <td> <b> Camada [L] <b> </td>\n",
        "        <td> $(m^{[L]}, m^{[L-1]})$ </td>\n",
        "        <td> $(m^{[L]}, 1)$ </td>\n",
        "        <td> $(m^{[L]}, 209)$  </td>\n",
        "    <tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "TuyWSfsFjekr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1 Funções de Ativação**\n",
        "\n",
        "#### **2.1.1 Sigmoide**\n",
        "\n",
        "O problema de aprendizado neste projeto é classificação binária. Portanto, usaremos a função de ativação sigmoide na camada de saída e a função de perda de Entropia Cruzada Binária (*Binary Cross-Entropy*). Dessa forma, implemente abaixo a função sigmoide. É importante ressaltar que além da ativação, a função também retorna o valor de Z, que chamamos de \"cache\", pois usaremos esse valor durante o cálculo do gradiente durante o *backpropagation*."
      ],
      "metadata": {
        "id": "WhQ6Acqq1cYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Implementa a ativação sigmoide em numpy\n",
        "\n",
        "    Argumentos:\n",
        "    Z -- array numpy de qualquer formato\n",
        "\n",
        "    Retorna:\n",
        "    A -- saída de sigmoide(z), mesmo formato que Z\n",
        "    cache -- retorna Z também, útil durante o backpropagation\n",
        "    \"\"\"\n",
        "    ### SEU CÓDIGO COMEÇA AQUI ### ≈1 linha\n",
        "\n",
        "    ### SEU CÓDIGO TERMINA AQUI ###\n",
        "    cache = Z\n",
        "\n",
        "    return A, cache"
      ],
      "metadata": {
        "id": "-Nt978ox8Wm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.2 Relu**\n",
        "\n",
        "Como visto em aula, ReLU é atualmente uma das escolhas mais populares para a função de ativação de neurônios em camadas ocultas. Neste projeto, também usaremos essa função nessas camadas.\n",
        "\n",
        "Implemente a função de ativação ReLU. Observe que, assim como a sigmoide, a função retorna tanto a ativação quanto o valor de Z. Lembre-se de que ReLU é definido por $relu(z) = max(0, z)$. Em numpy, a função max é implementada por `np.maximum`."
      ],
      "metadata": {
        "id": "UeC0Aoj08sWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Implemente a função RELU.\n",
        "\n",
        "    Argumentos:\n",
        "    Z -- Saída da camada linear, de qualquer formato\n",
        "\n",
        "    Retorna:\n",
        "    A -- Parâmetro pós-ativação, do mesmo formato que Z\n",
        "    cache -- um dicionário Python contendo \"A\"; armazenado para calcular o backpropagation de forma eficiente\n",
        "    \"\"\"\n",
        "    ### SEU CÓDIGO COMEÇA AQUI ### ≈1 linha\n",
        "\n",
        "    ### SEU CÓDIGO TERMINA AQUI ###\n",
        "    assert(A.shape == Z.shape)\n",
        "\n",
        "    cache = Z\n",
        "    return A, cache"
      ],
      "metadata": {
        "id": "EIv00d9_8XlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z = np.array([[-2.0 , 7.0]])\n",
        "A, cache = relu(Z)\n",
        "\n",
        "print(\"Ativação: \", A)\n",
        "print(\"Cache: \", cache)"
      ],
      "metadata": {
        "id": "dFFKNd6eNf5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultados Esperados:**\n",
        "<table style=\"width:50%\">\n",
        "  <tr>\n",
        "    <td><b> Ativação </b></td>\n",
        "    <td > [[ 0.  7.]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><b> Cache </b></td>\n",
        "    <td > [[ -2.  7.]] </td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "sl-VrLQaOCSk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6badpbTcmhQL"
      },
      "source": [
        "### **2.2 - Multilayer Perceptron (MLP)**\n",
        "Implemente a propagação direta de um MLP com L camadas. Para isso, implemente duas funções auxiliares: `linear_forward` e `linear_activation_forward`. A primeira calcula a parte linear $Z^{[l]}$ e a segunda calcula a ativação $g^{[l]}(Z^{[l]})$ da camada $l$. Isso facilitará a implementação do *backpropagation* nas próximas etapas."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1 Combinação Linear da Camada $l$**\n",
        "\n",
        "Implemente a combinação linear $Z^{[l]}$ entre os pesos e as entradas de uma camada $l$ do MLP. Lembre-se de que essa combinação é dada pela equação $Z^{[l]} = W^{[l]} A^{[l-1]} + \\mathbf{b}^{[l]}$."
      ],
      "metadata": {
        "id": "Bx-za7R2CD7V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9WDI8eZmhQL"
      },
      "outputs": [],
      "source": [
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implementa a parte linear da propagação direta de uma camada.\n",
        "\n",
        "    Argumentos:\n",
        "    A -- ativações da camada anterior (ou dados de entrada): (tamanho da camada anterior, número de exemplos)\n",
        "    W -- matriz de pesos: array numpy de tamanho (tamanho da camada atual, tamanho da camada anterior)\n",
        "    b -- vetor de viés, array numpy de tamanho (tamanho da camada atual, 1)\n",
        "\n",
        "    Retorna:\n",
        "    Z -- a entrada da função de ativação, também chamada de parâmetro de pré-ativação\n",
        "    cache -- um dicionário Python contendo \"A\", \"W\" e \"b\"; armazenado para calcular o backpropagation de forma eficiente\n",
        "    \"\"\"\n",
        "\n",
        "    ### SEU CÓDIGO COMEÇA AQUI ### ≈1 linha\n",
        "\n",
        "    ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "\n",
        "    return Z, cache"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "A = np.random.randn(3,2)\n",
        "W = np.random.randn(1,3)\n",
        "b = np.random.randn(1,1)\n",
        "Z, linear_cache = linear_forward(A, W, b)\n",
        "print(\"Z = \" + str(Z))"
      ],
      "metadata": {
        "id": "wmZvqYwyOYxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultados Esperados:**\n",
        "\n",
        "<table style=\"width:35%\">\n",
        "  \n",
        "  <tr>\n",
        "    <td><b> Z </b></td>\n",
        "    <td> [[ 3.26295337 -1.23429987]] </td>\n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ],
      "metadata": {
        "id": "bo5zr_QgOZam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2 Ativação da Camada $l$**\n",
        "\n",
        "Implemente a ativação $g^{[l]}(Z^{[l]})$ da camada $l$ do MLP. Embora a função de ativação ReLU em camadas ocultas torne o processo de aprendizado mais rápido do que a sigmoide, também suportaremos o uso de sigmoide nessas camadas, para que possamos conduzir experimentos com diferentes ativações. Lembre-se de que essa combinação é dada pela equação $g^{[l]}(Z^{[l]})$,\n",
        "onde $Z^{[l]}$ é calculado pela função `linear_forward` implementada na etapa anterior e $g^{[l]}$ pode ser `sigmoide` ou `relu`, dependendo\n",
        "do parâmetro `activation` (string)."
      ],
      "metadata": {
        "id": "dmNKH6pPC774"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR5jUje2mhQM"
      },
      "outputs": [],
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implementa a propagação direta para a camada LINEAR->ATIVAÇÃO\n",
        "\n",
        "    Argumentos:\n",
        "    A_prev -- ativações da camada anterior (ou dados de entrada): (tamanho da camada anterior, número de exemplos)\n",
        "    W -- matriz de pesos: matriz numpy de tamanho (tamanho da camada atual, tamanho da camada anterior)\n",
        "    b -- vetor de polarização, matriz numpy de tamanho (tamanho da camada atual, 1)\n",
        "    activation -- a ativação a ser usada nesta camada, armazenada como uma string de texto: \"sigmoid\" ou \"relu\"\n",
        "\n",
        "    Retorna:\n",
        "    A -- a saída da função de ativação, também chamada de valor pós-ativação\n",
        "    cache -- um dicionário Python contendo \"linear_cache\" e \"activation_cache\";\n",
        "    armazenado para calcular a passagem reversa de forma eficiente\n",
        "    \"\"\"\n",
        "\n",
        "    if activation == \"sigmoid\":\n",
        "        # Entradas: \"A_prev, W, b\". Saídas: \"A, activation_cache\".\n",
        "        ### SEU CÓDIGO COMEÇA AQUI ### ~2 linhas\n",
        "\n",
        "\n",
        "        ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "\n",
        "    elif activation == \"relu\":\n",
        "        # Entradas: \"A_prev, W, b\". Saídas: \"A, activation_cache\".\n",
        "        ### SEU CÓDIGO COMEÇA AQUI ### ~2 linhas\n",
        "\n",
        "\n",
        "        ### SEU CÓDIGO TERMINA AQUI ###\n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2)\n",
        "A_prev = np.random.randn(3,2); W = np.random.randn(1,3); b = np.random.randn(1,1)\n",
        "\n",
        "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
        "print(\"Ativação (sigmoid) = \" + str(A))\n",
        "\n",
        "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
        "print(\"Ativação (relu) = \" + str(A))"
      ],
      "metadata": {
        "id": "aoOs7R4LO--K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultados Esperados:**       \n",
        "<table style=\"width:35%\">\n",
        "  <tr>\n",
        "    <td> <b> Ativação (sigmoid) </b></td>\n",
        "    <td > [[ 0.96890023  0.11013289]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><b> Ativação (relu) </b></td>\n",
        "    <td > [[ 3.43896131  0.        ]]</td>\n",
        "  </tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "CexcNOWJPUv-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJrwrlewmhQM"
      },
      "source": [
        "### **2.3 Forward Propagation (Forward Pass)**\n",
        "\n",
        "A função `linear_activation_forward` calcula a ativação de uma única camada $l$. Use esta função para implementar a função `forward_pass`, que realiza a propagação das entradas $X$ por todas as camadas da rede. Esquematicamente, a arquitetura do MLP com L camadas pode ser vista da seguinte forma:\n",
        "\n",
        "$X$ $→$ (Linear | ReLU) $\\times$ [L-1] $→$ (Linear | Sigmóide) $→$ $\\hat{Y}$\n",
        "\n",
        "Primeiramente, calcule a ativação $A^{[l]}$ para as [L-1] camadas ocultas (Linear $→$ ReLU) e, em seguida, calcule a ativação $A^{[L]} = \\hat{Y}$ da camada de saída. Para calcular a ativação de uma camada $l$ com a função `linear_activation_forward`, você precisa dos pesos $W^{[l]}$ e $\\mathbf{b}^{[l]}$ dessa camada. A variável `parameters` da função `forward_pass` armazena os pesos de todas as camadas em um dicionário. Para acessar os pesos de uma determinada camada $l$, basta criar uma chave de string com o nome do parâmetro e o número da camada. Por exemplo, `parameters['W1']` e `parameters['b1']` contêm os parâmetros $W^{[1]}$ e $\\mathbf{b}^{[1]}$ da camada 1, respectivamente.\n",
        "\n",
        "Lembre-se de que, além da ativação $A^{[l]}$, a função `linear_activation_forward` retorna o valor de $Z^{[l]}$ em um cache. Como precisaremos desses valores durante o backprop, armazene (`cache.append()`) as saídas de cada camada na lista `caches`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0nWcsPcmhQM"
      },
      "outputs": [],
      "source": [
        "def forward_pass(X, parameters):\n",
        "    \"\"\"\n",
        "    Implementar propagação direta para o cálculo [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID\n",
        "\n",
        "    Argumentos:\n",
        "    X -- dados, array numpy de tamanho (tamanho da entrada, número de exemplos)\n",
        "    parâmetros -- saída de initialize_parameters_deep()\n",
        "\n",
        "    Retornos:\n",
        "    Y_hat -- último valor pós-ativação\n",
        "    caches -- lista de caches contendo:\n",
        "    todos os caches de linear_relu_forward() (há L-1 deles, indexados de 0 a L-2)\n",
        "    o cache de linear_sigmoid_forward() (há um, indexado L-1)\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2 # numeros de camadas na rede neural\n",
        "\n",
        "    # Implemente [LINEAR -> RELU]*(L-1). Adicione \"cache\" à lista \"caches\".\n",
        "    for l in range(1, L):\n",
        "        A_prev = A\n",
        "        ### SEU CÓDIGO COMEÇA AQUI ### ~2 linhas\n",
        "\n",
        "\n",
        "        ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "    # Implemente LINEAR -> SIGMOID. Adicione \"cache\" à lista \"caches\".\n",
        "    ### SEU CÓDIGO COMEÇA AQUI ### ~2 linhas\n",
        "\n",
        "\n",
        "    ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "    assert(Y_hat.shape == (1,X.shape[1]))\n",
        "\n",
        "    return Y_hat, caches"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(6)\n",
        "X = np.random.randn(5,4)\n",
        "W1 = np.random.randn(4,5); b1 = np.random.randn(4,1)\n",
        "W2 = np.random.randn(3,4); b2 = np.random.randn(3,1)\n",
        "W3 = np.random.randn(1,3); b3 = np.random.randn(1,1)\n",
        "parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3}\n",
        "\n",
        "Y_hat, caches = forward_pass(X, parameters)\n",
        "print(\"Y_hat = \" + str(Y_hat))\n",
        "print(\"Tamanho do cache = \" + str(len(caches)))"
      ],
      "metadata": {
        "id": "knW56iT9j7FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultados Esperados**       \n",
        "\n",
        "<table style=\"width:50%\">\n",
        "  <tr>\n",
        "    <td><b> Y_hat</b> </td>\n",
        "    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> <b>Tamanho do cache </b> </td>\n",
        "    <td > 3 </td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "cGiQ_aYGkZ8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Parte 3: Inicializar pesos**\n",
        "\n",
        "Ao contrário da regressão logística, em MLP não podemos inicializar os pesos $W^{[l]}$ de uma camada $l$ com zeros, pois isso tornaria os neurônios na mesma camada idênticos. Em MLP, os pesos são inicializados com valores aleatórios próximos de zero.\n",
        "\n",
        "Em aula, vimos uma solução simples que consistia em gerar uma matriz aleatória com valores entre 0 e 1 e multiplicá-la por uma constante pequena (por exemplo, 0,01). No entanto, uma solução melhor é gerar uma matriz aleatória com valores entre 0 e 1 e dividi-la pela raiz quadrada do número de entradas $m[^{[l-1]}]$ da camada $l$. Isso ajuda a garantir que a saída de um neurônio não seja dominada por um pequeno número de entradas, o que poderia levar a um sobreajuste. Quando um neurônio recebe um grande número de entradas, a saída pode ser altamente variável. Isso ocorre porque algumas entradas podem ter um impacto muito maior nos resultados do que outras. Por exemplo, se um neurônio recebe 100 entradas e uma delas é 10 vezes maior que as outras 99, essa entrada dominará a saída. Para evitar esse problema, os pesos das entradas são divididos pela raiz quadrada do número de entradas.\n",
        "\n",
        "Implemente a seguinte função para inicializar os pesos MLP usando essa técnica. Para gerar uma matriz aleatória com valores entre 0 e 1, use a função `np.random.randn(m_l, m_{l-1})`. Para calcular a raiz quadrada do número de entradas de uma camada, use a função `np.sqrt(m_{l-1})`. Os pesos $b^{[l]}$ podem ser inicializados com zero. Para isso, use a função `np.zeros((m_l, 1))`. Observe que esta função recebe como parâmetro uma lista `layer_dims` onde um elemento com índice $l$ contém o número de neurônios na camada $l$."
      ],
      "metadata": {
        "id": "FmqVtM8kGUjs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pe_I6GFUmhQL"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(layer_dims):\n",
        "    \"\"\"\n",
        "    Argumentos:\n",
        "    layer_dims -- array python (lista) contendo as dimensões de cada camada em nossa rede\n",
        "\n",
        "    Retorna:\n",
        "    parameters -- dicionário python contendo seus parâmetros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "    Wl -- matriz de pesos da forma (layer_dims[l], layer_dims[l-1])\n",
        "    bl -- vetor de polarização da forma (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims) # número de camadas na rede\n",
        "\n",
        "    for l in range(1, L):\n",
        "        ### SEU CÓDIGO COMEÇA AQUI ### ~2 linhas\n",
        "\n",
        "\n",
        "        ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_VzRJIsmhQL"
      },
      "outputs": [],
      "source": [
        "parameters = initialize_parameters([5,4,3])\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rI2TWJcmhQL"
      },
      "source": [
        "**Resultados Esperados:**       \n",
        "       \n",
        "<table style=\"width:80%\">\n",
        "  <tr>\n",
        "    <td><b>W1<b></td>\n",
        "    <td>[[ 0.72642933 -0.27358579 -0.23620559 -0.47984616  0.38702206]\n",
        " [-1.0292794   0.78030354 -0.34042208  0.14267862 -0.11152182]\n",
        " [ 0.65387455 -0.92132293 -0.14418936 -0.17175433  0.50703711]\n",
        " [-0.49188633 -0.07711224 -0.39259022  0.01887856  0.26064289]]</td>\n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td><b>b1<b> </td>\n",
        "    <td>[[ 0.]\n",
        " [ 0.]\n",
        " [ 0.]\n",
        " [ 0.]]</td>\n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td><b>W2<b></td>\n",
        "    <td>[[-0.55030959  0.57236185  0.45079536  0.25124717]\n",
        " [ 0.45042797 -0.34186393 -0.06144511 -0.46788472]\n",
        " [-0.13394404  0.26517773 -0.34583038 -0.19837676]]</td>\n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td><b>b2<b> </td>\n",
        "    <td>[[ 0.]\n",
        " [ 0.]\n",
        " [ 0.]]</td>\n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD2zeC3bmhQN"
      },
      "source": [
        "### **Parte 4: Defina a função de perda**\n",
        "\n",
        "Como temos um problema de classificação binária, usaremos a Entropia Cruzada Binária (*BCE*) como função de perda. Em MLP, o problema de otimização com esta função não é mais convexo, mas a descida do gradiente ainda funciona muito bem para otimizar os pesos da rede. Lembre-se de que a Entropia Cruzada Binária é definida da seguinte forma:\n",
        "\n",
        "$$\n",
        "L(h) = -\\frac{1}{m}\\sum_{i=1}^{m}(y_i\\;log\\ \\hat{y_i} + (1 - y_i)\\;log\\;(1 - \\hat{y_i})) \\tag{7}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDUmAXH_mhQN"
      },
      "outputs": [],
      "source": [
        "def binary_cross_entropy(Y_hat, Y):\n",
        "    \"\"\"\n",
        "    Implemente a função de custo definida pela equação (7).\n",
        "\n",
        "    Argumentos:\n",
        "    Y_hat -- vetor de probabilidade correspondente às suas previsões de rótulo, forma (1, número de exemplos)\n",
        "    Y -- vetor \"rótulo\" verdadeiro (por exemplo: contendo 0 se não for gato, 1 se gato), forma (1, número de exemplos)\n",
        "\n",
        "    Retorna:\n",
        "    custo -- custo de entropia cruzada\n",
        "    \"\"\"\n",
        "\n",
        "    m = Y.shape[1]\n",
        "    ### SEU CÓDIGO COMEÇA AQUI ### ~1 linha\n",
        "\n",
        "    ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "    loss = np.squeeze(loss) # Para garantir que o formato do seu custo seja o que esperamos (por exemplo, isso transforma [[17]] em 17).\n",
        "    assert(loss.shape == ())\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "y_hat = np.random.randn(5, 1)\n",
        "y = (np.random.randn(5, 1) > 0.3) * 1\n",
        "print(y_hat)\n",
        "print(y)\n",
        "binary_cross_entropy(y_hat, y)"
      ],
      "metadata": {
        "id": "bERRQehOBCOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JXqLogjmhQN"
      },
      "source": [
        "### **Parte 5: Retropropagação (Backpropagation)**\n",
        "\n",
        "Os pesos de um MLP são atualizados com o algoritmo de Retropropagação.\n",
        "Este algoritmo calcula eficientemente as derivadas parciais da função de erro em relação aos pesos da rede, aplicando a regra da cadeia de trás para frente, começando na camada de saída e retornando à camada de entrada. Como vimos em aula, cada nó em nosso grafo computacional precisa saber como calcular seus gradientes locais, que são as derivadas parciais de sua saída em relação às suas entradas. Nesta tarefa de programação, temos quatro nós em nosso grafo computacional: `linear(A, w, b)`, `sigmoid(z)`, `relu(z)` e `binary_cross_entropy(y_hat, y)`. Portanto, precisamos definir os gradientes locais desses quatro nós. Como já calculamos as derivadas parciais da perda de entropia cruzada binária para esses quatro nós em aula, calcularemos seus gradientes globais diretamente.\n",
        "\n",
        "Primeiro, você implementará a função `linear_backward` para calcular as derivadas parciais da função de perda $L$ em relação aos pesos ($W^{[l]}$, $b^{[l]}$) e à ativação da camada anterior $A^{[l-1]}$. Em segundo lugar, as funções `sigmoid_backward` e `relu_backward` para calcular as derivadas das funções de ativação em relação às entradas Z. Em seguida, você implementará a função `linear_activation_backward`, que combina essas funções para calcular a retropropagação para uma única camada $l$.\n",
        "Finalmente, você implementará a função `backward_pass`, que utiliza\n",
        "a função `linear_activation_backward` para calcular a retropropagação em todas as camadas do MLP."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.1 Derivadas Parciais da Combinação Linear da Camada $l$**\n",
        "\n",
        "Derivadas parciais da função de perda $L$ em relação aos pesos ($W^{[l]}$, $b^{[l]}$) de uma camada $l$ do MLP:\n",
        "\n",
        "$$ dW^{[l]} = \\frac{\\partial L}{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n",
        "$$ db^{[l]} = \\frac{\\partial L}{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)} $$\n",
        "$$ dA^{[l-1]} = \\frac{\\partial L}{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$"
      ],
      "metadata": {
        "id": "HSorv_L6HRpG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LgcfFKvmhQN"
      },
      "outputs": [],
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implementa a porção linear da propagação para trás para uma única camada (camada l)\n",
        "\n",
        "    Argumentos:\n",
        "    dZ -- Gradiente do custo em relação à saída linear (da camada l atual)\n",
        "    cache -- tupla de valores (A_prev, W, b) provenientes da propagação para frente na camada atual\n",
        "\n",
        "    Retorna:\n",
        "    dA_prev -- Gradiente do custo em relação à ativação (da camada l-1 anterior), mesmo formato que A_prev\n",
        "    dW -- Gradiente do custo em relação a W (camada l atual), mesmo formato que W\n",
        "    db -- Gradiente do custo em relação a b (camada l atual), mesmo formato que b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    ### SEU CÓDIGO COMEÇA AQUI ### ~3 linhas\n",
        "\n",
        "\n",
        "\n",
        "    ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "\n",
        "    return dA_prev, dW, db"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cria algumas amostras de testes\n",
        "np.random.seed(1)\n",
        "\n",
        "dZ = np.random.randn(3,4)\n",
        "A = np.random.randn(5,4)\n",
        "W = np.random.randn(3,5)\n",
        "b = np.random.randn(3,1)\n",
        "linear_cache = (A, W, b)\n",
        "\n",
        "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db))"
      ],
      "metadata": {
        "id": "svgo3Y29nqwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultados Esperados:**       \n",
        "       \n",
        "<table style=\"width:80%\">\n",
        "  <tr>\n",
        "    <td> <b> dA_prev </b> </td>\n",
        "    <td>[[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
        " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
        " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
        " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
        " [-2.52214926  2.67882552 -0.67947465  1.48119548]]</td>\n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td><b> dW </b> </td>\n",
        "    <td>[[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
        " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
        " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]</td>\n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td><b> db </b></td>\n",
        "    <td>[[-0.14713786]\n",
        " [-0.11313155]\n",
        " [-0.13209101]]</td>\n",
        "  </tr>\n",
        "\n",
        "  \n",
        "</table>"
      ],
      "metadata": {
        "id": "UiYFRAaXSgrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.2 Derivadas de funções de ativação**\n",
        "\n",
        "Derivadas da função sigmoide em relação à entrada Z:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial g}{\\partial Z} = \\frac{\\partial L}{\\partial a} \\cdot (g(z) \\cdot (1 - g(z))\n",
        "$$\n",
        "\n",
        "Caso tenha curiosidade, pode conferir como foi realizada a [derivação](https://math.stackexchange.com/a/1225116)"
      ],
      "metadata": {
        "id": "lewT9kBwIzXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implementa a propagação regressiva para uma única unidade SIGMOID.\n",
        "\n",
        "    Argumentos:\n",
        "    dA -- gradiente pós-ativação, de qualquer formato\n",
        "    cache -- onde armazenamos 'Z' para calcular a propagação regressiva de forma eficiente\n",
        "\n",
        "    Retorna:\n",
        "    dZ -- Gradiente do custo em relação a Z\n",
        "    \"\"\"\n",
        "    Z = cache\n",
        "\n",
        "    g_z = 1/(1 + np.exp(-Z))\n",
        "    ### SEU CÓDIGO COMEÇA AQUI ### ~1 linha\n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    assert (dZ.shape == Z.shape)\n",
        "\n",
        "    return dZ"
      ],
      "metadata": {
        "id": "jIg1pECRD6g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "dA = np.random.randn(3,4)\n",
        "cache = np.random.randn(3,4)\n",
        "\n",
        "dZ = sigmoid_backward(dA, cache)\n",
        "print('dZ: ', dZ)"
      ],
      "metadata": {
        "id": "Wg4D3A51TiGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultados Esperados:**       \n",
        "       \n",
        "<table style=\"width:80%\">\n",
        "  <tr>\n",
        "    <td> <b> dZ </b> </td>\n",
        "    <td>[[ 0.39571307 -0.14743535 -0.09728415 -0.20105294]\n",
        " [ 0.21475173 -0.47735759  0.43600867 -0.17501431]\n",
        " [ 0.05975979 -0.04567319  0.30026189 -0.48384433]]</td>\n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ],
      "metadata": {
        "id": "BIE57ao7ULpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Derivada da função ReLU em relação à entrada Z:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial g}{\\partial Z} = \\begin{cases}\n",
        "0\\, \\text{se}\\ z < 0 \\\\\n",
        "1\\, \\text{se}\\ z > 0 \\\\\n",
        "\\nexists, \\text{se}\\ z = 0 \\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Observe que, matematicamente, a derivada de ReLU é indefinida para $Z = 0$. Na prática, assumiremos que ela é igual a zero."
      ],
      "metadata": {
        "id": "MoqsqyaHRB6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implementa a propagação regressiva para uma única unidade RELU.\n",
        "\n",
        "    Argumentos:\n",
        "    dA -- gradiente pós-ativação, de qualquer formato\n",
        "    cache -- onde armazenamos 'Z' para calcular a propagação regressiva de forma eficiente\n",
        "\n",
        "    Retorna:\n",
        "    dZ -- Gradiente do custo em relação a Z\n",
        "    \"\"\"\n",
        "\n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # apenas convertendo dz para um objeto correto.\n",
        "\n",
        "    # Quando z <= 0, você deve setar dz para 0 também\n",
        "    ### SEU CÓDIGO COMEÇA AQUI ### ~1 linha\n",
        "\n",
        "    ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "    assert (dZ.shape == Z.shape)\n",
        "\n",
        "    return dZ"
      ],
      "metadata": {
        "id": "A6oxrtdfD4hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "dA = np.random.randn(3,4)\n",
        "cache = np.random.randn(3,4)\n",
        "\n",
        "dZ = relu_backward(dA, cache)\n",
        "print('dZ: ', dZ)"
      ],
      "metadata": {
        "id": "-7GmrypkUXrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultados Esperados:**       \n",
        "       \n",
        "<table style=\"width:80%\">\n",
        "  <tr>\n",
        "    <td> <b> dZ </b> </td>\n",
        "    <td>[[ 0.          0.         -0.52817175  0.        ]\n",
        " [ 0.          0.          1.74481176 -0.7612069 ]\n",
        " [ 0.         -0.24937038  1.46210794 -2.06014071]]</td>\n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ],
      "metadata": {
        "id": "towh0JcrUfaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.3 Derivadas Parciais da Ativação da Camada $l$**"
      ],
      "metadata": {
        "id": "GAMylWhNSKG_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "057W5sSCmhQO"
      },
      "outputs": [],
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implementa a propagação regressiva para a camada LINEAR->ATIVAÇÃO.\n",
        "\n",
        "    Argumentos:\n",
        "    dA -- gradiente pós-ativação para a camada atual l\n",
        "    cache -- tupla de valores (linear_cache, activation_cache) que armazenamos para calcular a propagação regressiva de forma eficiente\n",
        "    activation -- a ativação a ser usada nesta camada, armazenada como uma string de texto: \"sigmoid\" ou \"relu\"\n",
        "\n",
        "    Retorna:\n",
        "    dA_prev -- Gradiente do custo em relação à ativação (da camada anterior l-1), mesmo formato que A_prev\n",
        "    dW -- Gradiente do custo em relação a W (camada atual l), mesmo formato que W\n",
        "    db -- Gradiente do custo em relação a b (camada atual l), mesmo formato que b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        ### SEU CÓDIGO COMEÇA AQUI ### ~2 linhas\n",
        "\n",
        "\n",
        "        ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "    elif activation == \"sigmoid\":\n",
        "        ### SEU CÓDIGO COMEÇA AQUI ### ~2 linhas\n",
        "\n",
        "\n",
        "        ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "    return dA_prev, dW, db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPJuJGkHmhQO"
      },
      "source": [
        "### **5.4: Retropropagação**\n",
        "\n",
        "A função `linear_activation_backward` calcula a retropropagação para uma única camada $l$. Use esta função para implementar a função `backward_pass`, que realiza a retropropagação dos erros $L(\\hat{Y}, Y)$ por todas as camadas da rede, de trás para frente. Esquematicamente, o processo de retropropagação pode ser visto da seguinte forma:\n",
        "\n",
        "(Linear | ReLU) $\\times$ [L-1] $←$ (Linear | Sigmoid) $←$ $L(\\hat{Y}, Y)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2Q4P78-mhQP"
      },
      "outputs": [],
      "source": [
        "def backward_pass(Y_hat, Y, caches):\n",
        "    \"\"\"\n",
        "    Implemente a propagação regressiva para o grupo [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID\n",
        "\n",
        "    Argumentos:\n",
        "    Y_hat -- vetor de probabilidade, saída da propagação direta (L_model_forward())\n",
        "    Y -- vetor \"rótulo\" verdadeiro (contendo 0 se não for gato, 1 se for gato)\n",
        "    caches -- lista de caches contendo:\n",
        "        todos os caches de linear_activation_forward() com \"relu\" (há (L-1) ou mais, índices de 0 a L-2)\n",
        "        o cache de linear_activation_forward() com \"sigmoide\" (há um, índice L-1)\n",
        "\n",
        "    Retorna:\n",
        "    grads -- Um dicionário com os gradientes\n",
        "    grads[\"dA\" + str(l)] = ...\n",
        "    grads[\"dW\" + str(l)] = ...\n",
        "    grads[\"db\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # número de camadas\n",
        "    Y = Y.reshape(Y_hat.shape) # após esta linha, Y tem o mesmo formato de AL\n",
        "\n",
        "    # Calcule d_Yhat para inicializar a retropropagação\n",
        "    ### SEU CÓDIGO COMEÇA AQUI ### ~1 linha\n",
        "\n",
        "    ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "    # Gradientes da camada L (SIGMOID -> LINEAR). Entradas: \"Y_hat, Y, caches\". Saídas: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "    ### SEU CÓDIGO COMEÇA AQUI ### ~2 linhas\n",
        "\n",
        "\n",
        "    ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "    for l in reversed(range(1, L)):\n",
        "        # camada l-ésima: gradientes (RELU -> LINEAR).\n",
        "        ### SEU CÓDIGO COMEÇA AQUI ### ~5 linhas\n",
        "\n",
        "\n",
        "        ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(3)\n",
        "Y_hat = np.random.randn(1, 2); Y = np.array([[1, 0]])\n",
        "\n",
        "A1 = np.random.randn(4,2); W1 = np.random.randn(3,4); b1 = np.random.randn(3,1); Z1 = np.random.randn(3,2)\n",
        "linear_cache_activation_1 = ((A1, W1, b1), Z1)\n",
        "\n",
        "A2 = np.random.randn(3,2); W2 = np.random.randn(1,3); b2 = np.random.randn(1,1); Z2 = np.random.randn(1,2)\n",
        "linear_cache_activation_2 = ((A2, W2, b2), Z2)\n",
        "\n",
        "caches = (linear_cache_activation_1, linear_cache_activation_2)\n",
        "\n",
        "grads = backward_pass(Y_hat, Y, caches)\n",
        "\n",
        "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
        "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
        "print (\"dA1 = \"+ str(grads[\"dA1\"]))"
      ],
      "metadata": {
        "id": "0zN3Txs6cwFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultados Esperados:**       \n",
        "\n",
        "<table style=\"width:60%\">\n",
        "  <tr>\n",
        "    <td ><b> dW1 </b></td>\n",
        "           <td > [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
        " [0.         0.         0.         0.        ]\n",
        " [0.05283652 0.01005865 0.01777766 0.0135308 ]] </td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td > <b> db1 </b></td>\n",
        "           <td > [[-0.22007063]\n",
        " [ 0.        ]\n",
        " [-0.02835349]] </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "  <td > <b> dA1 </b></td>\n",
        "           <td > [[ 0.          0.52257901]\n",
        " [ 0.         -0.3269206 ]\n",
        " [ 0.         -0.32070404]\n",
        " [ 0.         -0.74079187]] </td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n"
      ],
      "metadata": {
        "id": "9Vb_GZjedlFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Parte 6: Otimizar pesos com gradiente descendente**\n",
        "\n",
        "Antes de implementar o loop principal de gradiente descendente, implemente uma função auxiliar `update_parameters` para atualizar os parâmetros de cada camada $l$ do MLP com seus respectivos gradientes. Lembre-se de que a regra de atualização do gradiente descendente é a seguinte:\n",
        "\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$\n",
        "\n",
        "onde $\\alpha$ é a taxa de aprendizado. Lembre-se de que todos os parâmetros do MLP são armazenados no dicionário `parameters` e os gradientes no dicionário `grads`."
      ],
      "metadata": {
        "id": "l3oj04bjgVKo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-N2Gu3GmhQP"
      },
      "outputs": [],
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Atualiza os parâmetros usando gradiente descendente\n",
        "\n",
        "    Argumentos:\n",
        "    parameters -- dicionário Python contendo seus parâmetros\n",
        "    grads -- dicionário Python contendo seus gradientes, saída de L_model_backward\n",
        "\n",
        "    Retorna:\n",
        "    parameters -- dicionário Python contendo seus parâmetros atualizados\n",
        "        parameters[\"W\" + str(l)] = ...\n",
        "        parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "\n",
        "    L = len(parameters) // 2 # números de camadas na rede\n",
        "\n",
        "    # regra de atualização para cada parâmetro. Use um loop for.\n",
        "    ### SEU CÓDIGO COMEÇA AQUI ### ~3 linhas\n",
        "\n",
        "\n",
        "\n",
        "    ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2)\n",
        "W1 = np.random.randn(3,4); b1 = np.random.randn(3,1)\n",
        "W2 = np.random.randn(1,3); b2 = np.random.randn(1,1)\n",
        "parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
        "\n",
        "np.random.seed(3)\n",
        "dW1 = np.random.randn(3,4); db1 = np.random.randn(3,1)\n",
        "dW2 = np.random.randn(1,3); db2 = np.random.randn(1,1)\n",
        "grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
        "\n",
        "parameters = update_parameters(parameters, grads, 0.1)\n",
        "\n",
        "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
        "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
        "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
        "print (\"b2 = \"+ str(parameters[\"b2\"]))"
      ],
      "metadata": {
        "id": "4drZvdHBhQ-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultados Esperados:**       \n",
        "\n",
        "<table style=\"width:100%\">\n",
        "    <tr>\n",
        "    <td > <b> W1 </b></td>\n",
        "           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
        " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
        " [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td > <b> b1 </b></td>\n",
        "           <td > [[-0.04659241]\n",
        " [-1.28888275]\n",
        " [ 0.53405496]] </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td > <b> W2 </b> </td>\n",
        "           <td > [[-0.55569196  0.0354055   1.32964895]]</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td > <b> b2 </b> </td>\n",
        "           <td > [[-0.84610769]] </td>\n",
        "  </tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "Y8ge6K4ShkqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemente o loop principal de descida do gradiente usando as seguintes funções implementadas ao longo deste projeto: `initialize_parameters`, `forward_pass`, `binary_cross_entropy`, `backward_pass` e `update_parameters`."
      ],
      "metadata": {
        "id": "MxX-APMXhIMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3-9Kz4GmrH0"
      },
      "outputs": [],
      "source": [
        "def optimize(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Implementa uma rede neural de L camadas: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
        "\n",
        "    Argumentos:\n",
        "    X -- dados, array numpy de formato (num_px * num_px * 3, número de exemplos)\n",
        "    Y -- vetor \"rótulo\" verdadeiro (contendo 0 se cat, 1 se não cat), de formato (1, número de exemplos)\n",
        "    layers_dims -- lista contendo o tamanho da entrada e o tamanho de cada camada, de comprimento (número de camadas + 1).\n",
        "    learning_rate -- taxa de aprendizado da regra de atualização de gradiente descendente\n",
        "    num_iterations -- número de iterações do loop de otimização\n",
        "    print_cost -- se True, imprime o custo a cada 100 passos\n",
        "\n",
        "    Retorna:\n",
        "    parameters -- parâmetros aprendidos pelo modelo. Eles podem então ser usados ​​para prever.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "    losses = []                         # mantém o histórico das perdas\n",
        "\n",
        "    # inicializa os parâmetros\n",
        "    ### SEU CÓDIGO COMEÇA AQUI ### ~1 linha\n",
        "\n",
        "    ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "    # laço (descida de gradiente)\n",
        "    for i in range(0, num_iterations):\n",
        "        # Propagação para frente: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
        "        ### SEU CÓDIGO COMEÇA AQUI ### ~1 linha\n",
        "\n",
        "        ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "        # calcula a perda.\n",
        "        ### SEU CÓDIGO COMEÇA AQUI ### ~1 linha\n",
        "\n",
        "        ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "        # Retropropagração.\n",
        "        ### SEU CÓDIGO COMEÇA AQUI ### ~1 linha\n",
        "\n",
        "        ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "        # atualiza os parâmetros.\n",
        "        ### SEU CÓDIGO COMEÇA AQUI ### ~1 linha\n",
        "\n",
        "        ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "        # imprime a função de perda a cada 100 exemplos de treino\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Loss após a iteração %i: %f\" %(i, loss))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            losses.append(loss)\n",
        "\n",
        "    return parameters, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVRewwpNmrH0"
      },
      "source": [
        "Em seguida, você usará a função `optimize` para treinar um MLP de 2 camadas e traçar sua curva de aprendizado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "Eb6Nd_A4mrH1"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.0075\n",
        "layers_dims = [12288, 7, 1]\n",
        "\n",
        "parameters_2layers, losses_2layers = optimize(train_set_x, train_set_y, layers_dims, learning_rate, num_iterations = 2500, print_cost = True)\n",
        "\n",
        "# visualiza o custo\n",
        "plt.plot(np.squeeze(losses_2layers))\n",
        "plt.ylabel('custo')\n",
        "plt.xlabel('iterações (por centena)')\n",
        "plt.title(\"Taxa de Aprendizado =\" + str(learning_rate))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultados Esperados:**       \n",
        "<table>\n",
        "    <tr>\n",
        "        <td> <b> Custo após iteração 0 </b></td>\n",
        "        <td> 0.695046 </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td> <b>Custo após iteração 100 </b></td>\n",
        "        <td> 0.589260 </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td> <b>...</b></td>\n",
        "        <td> ... </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td> <b> Custo após iteração 2400 </b></td>\n",
        "        <td> 0.026615 </td>\n",
        "    </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "3WRuaOYLkBla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora use a mesma função `optimize` para treinar um MLP de 4 camadas e traçar sua curva de aprendizado."
      ],
      "metadata": {
        "id": "_XLOOHvgjZ8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  modelo com 4-camadas\n",
        "learning_rate = 0.0075\n",
        "layers_dims = [12288, 20, 7, 5, 1]\n",
        "\n",
        "parameters_4layers, losses_4layers = optimize(train_set_x, train_set_y, layers_dims, learning_rate, num_iterations = 2500, print_cost = True)\n",
        "\n",
        "# visualizar o custo\n",
        "plt.plot(np.squeeze(losses_4layers))\n",
        "plt.ylabel('custo')\n",
        "plt.xlabel('iterações (por centena)')\n",
        "plt.title(\"Taxa de Aprendizado =\" + str(learning_rate))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5pFAiienet4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9XNJRQamrH1"
      },
      "source": [
        "**Resultados Esperados:**       \n",
        "<table>\n",
        "    <tr>\n",
        "        <td> <b> Cost after iteration 0 </b></td>\n",
        "        <td> 0.771749 </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td> <b> Cost after iteration 100 </b></td>\n",
        "        <td> 0.672053 </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td> <b>...</b></td>\n",
        "        <td> ... </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td> <b> Cost after iteration 2400</b></td>\n",
        "        <td> 0.092878 </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Parte 7: Avaliar modelos treinados**\n",
        "\n",
        "Finalmente, aplique o *threshold*   $ \\ \\hat{y} =\n",
        "\\begin{cases}\n",
        "1,\\ \\text{if}\\ h(x) \\geq 0.5\\\\\n",
        "0,\\ \\text{if}\\ h(x) < 0.5\\\\\n",
        "\\end{cases}$ para classificar todos os exemplos em uma matriz $X$ de exemplos."
      ],
      "metadata": {
        "id": "dWZSTNfAYxXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, y, parameters):\n",
        "    \"\"\"\n",
        "    Esta função é usada para prever os resultados de uma rede neural de camadas L.\n",
        "\n",
        "    Argumentos:\n",
        "    X -- conjunto de dados de exemplos que você gostaria de rotular\n",
        "    parameters -- parâmetros do modelo treinado\n",
        "\n",
        "    Retorna:\n",
        "    p -- previsões para o conjunto de dados X fornecido\n",
        "    accuracy -- porcentagem de exemplos rotulados corretamente\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2 # número de camadas na rede\n",
        "    p = np.zeros((1,m))\n",
        "\n",
        "    # propagação das entradas\n",
        "    ### SEU CÓDIGO COMEÇA AQUI ### ~1 linha\n",
        "\n",
        "    ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "    # converte as probabilidade para predições 0/1\n",
        "    ### SEU CÓDIGO COMEÇA AQUI ### ~1-2 linhas\n",
        "\n",
        "    ### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "    # calcula a acurácia\n",
        "    ### SEU CÓDIGO COMEÇA AQUI ### ~1 line\n",
        "\n",
        "    ### SEU CÓDIGO TERMINA AQUI ###\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "ZhghYM1QsIZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "UQ6sIaKAmrH1"
      },
      "outputs": [],
      "source": [
        "accuracy_train_2layers = predict(train_set_x, train_set_y, parameters_2layers)\n",
        "accuracy_test_2layers = predict(test_set_x, test_set_y, parameters_2layers)\n",
        "\n",
        "accuracy_train_4layers = predict(train_set_x, train_set_y, parameters_4layers)\n",
        "accuracy_test_4layers = predict(test_set_x, test_set_y, parameters_4layers)\n",
        "\n",
        "print(\"Acurácia do Modelo de 2 camadas no Treino:\", accuracy_train_2layers)\n",
        "print(\"Acurácia do Modelo de 2 camadas no Teste:\", accuracy_test_2layers)\n",
        "print(\"Acurácia do Modelo de 4 camadas no Treino:\", accuracy_train_4layers)\n",
        "print(\"Acurácia do Modelo de 4 camadas no Teste:\", accuracy_test_4layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCUTZ-TvmrH1"
      },
      "source": [
        "**Resultados Esperados:**       \n",
        "<table>\n",
        "    <tr>\n",
        "    <td>\n",
        "    <b>Acurácia do Modelo de 2 camadas no Treino</b>\n",
        "    </td>\n",
        "    <td>\n",
        "    1.0\n",
        "    </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "    <td> <b>Acurácia do Modelo de 2 camadas no Teste </b></td>\n",
        "    <td> 0.74 </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "    <td> <b>Acurácia do Modelo de 4 camadas no Treino</b></td>\n",
        "    <td> 0.9856459330143541 </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "    <td> <b>Acurácia do Modelo de 4 camadas no Teste</b></td>\n",
        "    <td> 0.8 </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsgiE9YDmrH1"
      },
      "source": [
        "Parabéns! Seu MLP de 4 camadas teve um desempenho melhor (80%) do que seu MLP de 2 camadas (74%) quando avaliado com seus dados de teste. Este é um bom desempenho!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Parte 8: Teste com suas próprias imagens**\n",
        "\n",
        "Para testar seu modelo com suas próprias imagens, você precisará enviar as imagens que deseja testar para o Google Drive. Em seguida, configure a variável `image_path` na célula abaixo para apontar para o caminho da imagem no seu Google Drive. Observe que o caminho para o diretório raiz do seu Google Drive é `/content/gdrive/MyDrive/`. Portanto, se a sua imagem estiver na raiz do seu Google Drive e se chamar \"my_cat.jpg\", a variável deverá ser configurada como `image_path = /content/gdrive/MyDrive/my_cat.jpg`."
      ],
      "metadata": {
        "id": "P5lKU_WgCAvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "### SEU CÓDIGO COMEÇA AQUI ### ~1 linha\n",
        "image_path = \"/content/gdrive/MyDrive/<caminho_para_imagem>.jpg\"\n",
        "### SEU CÓDIGO TERMINA AQUI ###\n",
        "\n",
        "\n",
        "# faz um pré-processamento na imagem para estar compatível com\n",
        "# o formato de entrada do seu algoritmo\n",
        "with Image.open(image_path) as im:\n",
        "  low_res_image = np.array(im.resize((num_px, num_px)))\n",
        "\n",
        "x = low_res_image.reshape((num_px * num_px * 3, 1)) # converte em um dado 1D\n",
        "x = x / 255 # normaliza os valores\n",
        "\n",
        "prob_y, _ = forward_pass(x, parameters_4layers)\n",
        "y = (prob_y > 0.5) * 1\n",
        "\n",
        "plt.imshow(low_res_image)\n",
        "print (\"y = \" + str(np.squeeze(y)) + \", seu algoritmo prediz que é uma imagem de '\" + classes[int(np.squeeze(y)),].decode(\"utf-8\")+  \"'.\")"
      ],
      "metadata": {
        "id": "Hc_vxmsECIG7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}